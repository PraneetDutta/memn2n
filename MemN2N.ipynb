{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting single.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile single.py\n",
    "\"\"\"Example running MemN2N on a single bAbI task.\n",
    "Download tasks from facebook.ai/babi \"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from data_utils import load_task, vectorize_data\n",
    "from sklearn import cross_validation, metrics\n",
    "from memn2n import MemN2N\n",
    "from itertools import chain\n",
    "from six.moves import range\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"epsilon\", 1e-8, \"Epsilon value for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 200, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\n",
    "tf.flags.DEFINE_integer(\"memory_size\", 50, \"Maximum size of memory.\")\n",
    "tf.flags.DEFINE_integer(\"task_id\", 1, \"bAbI task id, 1 <= id <= 20\")\n",
    "tf.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"../code/data/babi/tasks_1-20_v1-2/en/\", \"Directory containing bAbI tasks\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def get_log_dir_name():\n",
    "    lr = FLAGS.learning_rate\n",
    "    eps = FLAGS.epsilon\n",
    "    mgn = FLAGS.max_grad_norm\n",
    "    hp = FLAGS.hops\n",
    "    es = FLAGS.embedding_size\n",
    "    ms = FLAGS.memory_size\n",
    "    ti = FLAGS.task_id\n",
    "    \n",
    "    log_dir_name = \"lr={0}_eps={1}_mgn={2}_hp={3}_es={4}_ms={5}_ti={6}\".format(lr, eps, mgn, hp, es, ms, ti)\n",
    "    return os.path.join('./logs', log_dir_name)\n",
    "\n",
    "print(\"Started Task:\", FLAGS.task_id)\n",
    "\n",
    "# task data\n",
    "train, test = load_task(FLAGS.data_dir, FLAGS.task_id)\n",
    "data = train + test\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "mean_story_size = int(np.mean(map(len, (s for s, _, _ in data))))\n",
    "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "query_size = max(map(len, (q for _, q, _ in data)))\n",
    "memory_size = min(FLAGS.memory_size, max_story_size)\n",
    "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "sentence_size = max(query_size, sentence_size) # for the position\n",
    "\n",
    "print(\"Longest sentence length\", sentence_size)\n",
    "print(\"Longest story length\", max_story_size)\n",
    "print(\"Average story length\", mean_story_size)\n",
    "\n",
    "# train/validation/test sets\n",
    "S, Q, A = vectorize_data(train, word_idx, sentence_size, memory_size)\n",
    "trainS, valS, trainQ, valQ, trainA, valA = cross_validation.train_test_split(S, Q, A, test_size=.1, random_state=FLAGS.random_state)\n",
    "testS, testQ, testA = vectorize_data(test, word_idx, sentence_size, memory_size)\n",
    "\n",
    "print(testS[0])\n",
    "\n",
    "print(\"Training set shape\", trainS.shape)\n",
    "\n",
    "# params\n",
    "n_train = trainS.shape[0]\n",
    "n_test = testS.shape[0]\n",
    "n_val = valS.shape[0]\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)\n",
    "print(\"Testing Size\", n_test)\n",
    "\n",
    "train_labels = np.argmax(trainA, axis=1)\n",
    "test_labels = np.argmax(testA, axis=1)\n",
    "val_labels = np.argmax(valA, axis=1)\n",
    "\n",
    "tf.set_random_seed(FLAGS.random_state)\n",
    "batch_size = FLAGS.batch_size\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate, epsilon=FLAGS.epsilon)\n",
    "\n",
    "\n",
    "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "with tf.Session() as sess:\n",
    "    model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, FLAGS.embedding_size, session=sess,\n",
    "                   hops=FLAGS.hops, max_grad_norm=FLAGS.max_grad_norm, nonlin=tf.nn.relu, optimizer=optimizer)\n",
    "    \n",
    "    writer = tf.train.SummaryWriter(get_log_dir_name(), sess.graph)\n",
    "    \n",
    "    for t in range(1, FLAGS.epochs+1):\n",
    "        np.random.shuffle(batches)\n",
    "        total_cost = 0.0\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = start + batch_size\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            a = trainA[start:end]\n",
    "            cost_t, cost_summary = model.batch_fit(s, q, a)\n",
    "            total_cost += cost_t\n",
    "            \n",
    "            writer.add_summary(cost_summary, t*n_train+start)\n",
    "\n",
    "        if t % FLAGS.evaluation_interval == 0:\n",
    "            train_preds = []\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = start + batch_size\n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                train_preds += list(pred)\n",
    "\n",
    "#             val_preds = model.predict(valS, valQ)\n",
    "            train_acc = metrics.accuracy_score(np.array(train_preds), train_labels)\n",
    "#             val_acc = metrics.accuracy_score(val_preds, val_labels)\n",
    "\n",
    "            val_acc, val_acc_summary = model.get_val_acc_summary(valS, valQ, val_labels)\n",
    "            writer.add_summary(val_acc_summary, t)\n",
    "            \n",
    "            print('-----------------------')\n",
    "            print('Epoch', t)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print('Training Accuracy:', train_acc)\n",
    "            print('Validation Accuracy:', val_acc)\n",
    "            print('-----------------------')\n",
    "\n",
    "    test_preds = model.predict(testS, testQ)\n",
    "    test_acc = metrics.accuracy_score(test_preds, test_labels)\n",
    "    print(\"Testing Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
